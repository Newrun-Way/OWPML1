"""
RAGAS ê¸°ë°˜ RAG ì‹œìŠ¤í…œ í‰ê°€
"""

import json
import time
from pathlib import Path
from typing import List, Dict
import numpy as np
from loguru import logger

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from rag.pipeline import RAGPipeline

# RAGAS ê´€ë ¨ ì„í¬íŠ¸
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,          # ë‹µë³€ì´ ë¬¸ì„œì— ì¶©ì‹¤í•œê°€? (í™˜ê° ë°©ì§€)
        answer_relevancy,      # ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ê°€?
        context_precision,     # ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆœìœ„ê°€ ì •í™•í•œê°€?
        context_recall,        # í•„ìš”í•œ ì •ë³´ë¥¼ ëª¨ë‘ ê²€ìƒ‰í–ˆëŠ”ê°€?
        answer_correctness,    # ë‹µë³€ì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?
    )
    from datasets import Dataset
    RAGAS_AVAILABLE = True
except ImportError:
    RAGAS_AVAILABLE = False
    logger.warning("RAGASê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install ragas datasets ì‹¤í–‰ í•„ìš”")


class RAGASEvaluator:
    """RAGAS ê¸°ë°˜ RAG í‰ê°€ í´ë˜ìŠ¤"""
    
    def __init__(self, test_cases_path: str = "tests/test_cases.json"):
        """
        Args:
            test_cases_path: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ JSON íŒŒì¼ ê²½ë¡œ
        """
        if not RAGAS_AVAILABLE:
            raise ImportError("RAGASë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: pip install ragas datasets")
        
        self.test_cases = self._load_test_cases(test_cases_path)
        self.pipeline = RAGPipeline(use_structure_chunking=True)
        
        logger.info(f"RAGAS í‰ê°€ ì´ˆê¸°í™”: {len(self.test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
    
    def _load_test_cases(self, path: str) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ íŒŒì¼ ì—†ìŒ: {path}, ê¸°ë³¸ ì¼€ì´ìŠ¤ ì‚¬ìš©")
            return self._get_default_test_cases()
    
    def _get_default_test_cases(self) -> List[Dict]:
        """ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤"""
        return [
            {
                "id": "test_001",
                "question": "ê¸‰ì—¬ëŠ” ì–¸ì œ ì§€ê¸‰ë˜ë‚˜ìš”?",
                "ground_truth": "ê¸‰ì—¬ëŠ” ë§¤ì›” 25ì¼ì— ì§€ê¸‰í•˜ë©°, ì§€ê¸‰ì¼ì´ í† ìš”ì¼ ë˜ëŠ” ê³µíœ´ì¼ì¸ ê²½ìš° ê·¸ ì „ì¼ì— ì§€ê¸‰í•œë‹¤."
            },
            {
                "id": "test_002",
                "question": "ê³¼ì¥ì˜ ê¸°ë³¸ê¸‰ì€ ì–¼ë§ˆì¸ê°€ìš”?",
                "ground_truth": "ê³¼ì¥ì˜ ê¸°ë³¸ê¸‰ì€ 3,500,000ì›ì´ë‹¤."
            },
            {
                "id": "test_003",
                "question": "ì œ3ì¥ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ground_truth": "ì œ3ì¥ì—ì„œëŠ” ê¸‰ì—¬ ë° ìˆ˜ë‹¹ì— ê´€í•œ ë‚´ìš©ì„ ë‹¤ë£¬ë‹¤."
            },
            {
                "id": "test_004",
                "question": "ì§ì±…ìˆ˜ë‹¹ì€ ëˆ„êµ¬ì—ê²Œ ì§€ê¸‰ë˜ë‚˜ìš”?",
                "ground_truth": "ì§ì±…ìˆ˜ë‹¹ì€ ê³¼ì¥ê¸‰ ì´ìƒ ì§ì›ì—ê²Œ ì§€ê¸‰ëœë‹¤."
            },
            {
                "id": "test_005",
                "question": "ì•¼ê°„ê·¼ë¬´ìˆ˜ë‹¹ì€ ì‹œê°„ë‹¹ ì–¼ë§ˆì¸ê°€ìš”?",
                "ground_truth": "ì•¼ê°„ê·¼ë¬´ìˆ˜ë‹¹ì€ 22ì‹œ ì´í›„ ê·¼ë¬´ ì‹œ ì‹œê°„ë‹¹ 15,000ì›ì´ë‹¤."
            }
        ]
    
    def prepare_ragas_dataset(self, measure_response_time: bool = True) -> Dict:
        """
        RAGAS í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ì¤€ë¹„
        
        Returns:
            {
                "question": [...],
                "contexts": [[...], ...],
                "answer": [...],
                "ground_truths": [[...], ...]
            }
        """
        logger.info("RAGAS ë°ì´í„°ì…‹ ì¤€ë¹„ ì¤‘...")
        
        questions = []
        contexts_list = []
        answers = []
        ground_truths = []
        response_times = []
        
        for i, case in enumerate(self.test_cases):
            logger.info(f"  [{i+1}/{len(self.test_cases)}] {case['question']}")
            
            # RAG íŒŒì´í”„ë¼ì¸ ì§ˆì˜ (ì‘ë‹µ ì‹œê°„ ì¸¡ì •)
            if measure_response_time:
                start_time = time.time()
                result = self.pipeline.query(case["question"], top_k=5)
                elapsed = time.time() - start_time
                response_times.append(elapsed)
            else:
                result = self.pipeline.query(case["question"], top_k=5)
            
            # ë°ì´í„° ìˆ˜ì§‘
            questions.append(case["question"])
            answers.append(result.get("answer", ""))
            ground_truths.append([case.get("ground_truth", "")])
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ (contexts)
            contexts = []
            for source in result.get("sources", []):
                content = source.get("content", "")
                if content:
                    contexts.append(content)
            contexts_list.append(contexts if contexts else ["ì •ë³´ ì—†ìŒ"])
        
        logger.info("ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ")
        
        result = {
            "question": questions,
            "contexts": contexts_list,
            "answer": answers,
            "ground_truths": ground_truths
        }
        
        if measure_response_time:
            result["response_times"] = response_times
        
        return result
    
    def evaluate(
        self,
        metrics: List = None,
        save_results: bool = True,
        include_response_time: bool = True
    ) -> Dict:
        """
        RAGAS í‰ê°€ ì‹¤í–‰
        
        Args:
            metrics: í‰ê°€ ì§€í‘œ ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ answer_relevancyë§Œ ì‚¬ìš©)
            save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            include_response_time: ì‘ë‹µ ì†ë„ ì¸¡ì • í¬í•¨ ì—¬ë¶€
        
        Returns:
            í‰ê°€ ê²°ê³¼
        """
        # ê¸°ë³¸ ì§€í‘œ ì„¤ì • (ì‹ ë¢°ì„± + ê´€ë ¨ì„±)
        if metrics is None:
            metrics = [
                faithfulness,      # ì‹ ë¢°ì„± (í™˜ê° ë°©ì§€)
                answer_relevancy,  # ë‹µë³€ ê´€ë ¨ì„±
            ]
        
        self.include_response_time = include_response_time
        
        logger.info("=== RAGAS í‰ê°€ ì‹œì‘ ===")
        logger.info(f"í‰ê°€ ì§€í‘œ: {[m.name for m in metrics]}")
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        data = self.prepare_ragas_dataset(measure_response_time=include_response_time)
        
        # ì‘ë‹µ ì‹œê°„ ì €ì¥ (RAGAS ë°ì´í„°ì…‹ì—ì„œ ì œì™¸)
        response_times = data.pop("response_times", None)
        
        dataset = Dataset.from_dict(data)
        
        # RAGAS í‰ê°€ ì‹¤í–‰
        logger.info("\nRAGAS í‰ê°€ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        start_time = time.time()
        
        try:
            result = evaluate(dataset, metrics=metrics)
            evaluation_time = time.time() - start_time
            
            logger.info(f"í‰ê°€ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {evaluation_time:.1f}ì´ˆ)")
            
            # ê²°ê³¼ ì •ë¦¬
            scores = {
                "evaluation_time": evaluation_time,
                "test_count": len(self.test_cases),
                "metrics": {}
            }
            
            # ì§€í‘œë³„ ì ìˆ˜ ì¶”ì¶œ
            for metric in metrics:
                metric_name = metric.name
                if metric_name in result:
                    scores["metrics"][metric_name] = float(result[metric_name])
            
            # ì‘ë‹µ ì‹œê°„ í†µê³„ ì¶”ê°€
            if response_times:
                scores["response_time"] = {
                    "average": float(np.mean(response_times)),
                    "min": float(np.min(response_times)),
                    "max": float(np.max(response_times)),
                    "median": float(np.median(response_times)),
                    "per_query": response_times
                }
            
            # í‰ê·  ì ìˆ˜ ê³„ì‚°
            if scores["metrics"]:
                scores["average_score"] = np.mean(list(scores["metrics"].values()))
            
            # ê²°ê³¼ ì¶œë ¥
            self._print_results(scores)
            
            # ê²°ê³¼ ì €ì¥
            if save_results:
                self._save_results(scores, data)
            
            return scores
            
        except Exception as e:
            logger.error(f"RAGAS í‰ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    def _print_results(self, scores: Dict):
        """ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*70)
        print("RAGAS í‰ê°€ ê²°ê³¼")
        print("="*70)
        print(f"\nì´ í…ŒìŠ¤íŠ¸: {scores['test_count']}ê°œ")
        print(f"í‰ê°€ ì‹œê°„: {scores['evaluation_time']:.1f}ì´ˆ")
        
        # ì‘ë‹µ ì‹œê°„ ì¶œë ¥
        if "response_time" in scores:
            rt = scores["response_time"]
            print(f"\nì‘ë‹µ ì†ë„:")
            print("-"*70)
            print(f"  {'í‰ê·  ì‘ë‹µ ì‹œê°„':25s}: {rt['average']:.2f}ì´ˆ")
            print(f"  {'ìµœì†Œ ì‘ë‹µ ì‹œê°„':25s}: {rt['min']:.2f}ì´ˆ")
            print(f"  {'ìµœëŒ€ ì‘ë‹µ ì‹œê°„':25s}: {rt['max']:.2f}ì´ˆ")
            print(f"  {'ì¤‘ì•™ê°’':25s}: {rt['median']:.2f}ì´ˆ")
            
            # ì‘ë‹µ ì‹œê°„ íŒì •
            avg_time = rt['average']
            if avg_time < 2.0:
                time_status = "âœ… ìš°ìˆ˜"
            elif avg_time < 3.0:
                time_status = "âš ï¸ ë³´í†µ"
            else:
                time_status = "âŒ ê°œì„  í•„ìš”"
            print(f"  {'íŒì •':25s}: {time_status}")
        
        print(f"\ní‰ê°€ ì§€í‘œ:")
        print("-"*70)
        
        for metric_name, score in scores["metrics"].items():
            # ì§€í‘œë³„ íŒì •
            if score >= 0.85:
                status = "âœ… ìš°ìˆ˜"
            elif score >= 0.70:
                status = "âš ï¸ ë³´í†µ"
            else:
                status = "âŒ ê°œì„  í•„ìš”"
            
            print(f"  {metric_name:25s}: {score:.3f}  {status}")
        
        if "average_score" in scores:
            print("-"*70)
            print(f"  {'í‰ê·  ì ìˆ˜':25s}: {scores['average_score']:.3f}")
        
        print("="*70)
        
        # í•´ì„ ê°€ì´ë“œ
        print("\n[ì§€í‘œ í•´ì„ ê°€ì´ë“œ]")
        print("  â€¢ Faithfulness (ì‹ ë¢°ì„±):      ë‹µë³€ì´ ë¬¸ì„œì— ì¶©ì‹¤í•œê°€? (í™˜ê° ë°©ì§€)")
        print("  â€¢ Answer Relevancy (ê´€ë ¨ì„±):  ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ìˆëŠ”ê°€?")
        print("  â€¢ Context Precision (ì •ë°€ë„): ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ìˆœìœ„ê°€ ì •í™•í•œê°€?")
        print("  â€¢ Context Recall (ì¬í˜„ìœ¨):    í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ ê²€ìƒ‰í–ˆëŠ”ê°€?")
        print("  â€¢ Answer Correctness (ì •í™•ë„): ë‹µë³€ì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?")
        
        print("\n[íŒì • ê¸°ì¤€]")
        print("  âœ… ìš°ìˆ˜ (0.85+):    í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ")
        print("  âš ï¸ ë³´í†µ (0.70-0.85): íŒŒë¼ë¯¸í„° íŠœë‹ ê¶Œì¥")
        print("  âŒ ê°œì„  í•„ìš” (<0.70): ì‹œìŠ¤í…œ ê°œì„  í•„ìš”")
    
    def _save_results(self, scores: Dict, raw_data: Dict):
        """ê²°ê³¼ ì €ì¥"""
        output_path = Path("tests/ragas_evaluation_results.json")
        
        results = {
            "summary": scores,
            "raw_data": {
                "questions": raw_data["question"],
                "answers": raw_data["answer"],
                "ground_truths": raw_data["ground_truths"],
                "contexts_count": [len(ctx) for ctx in raw_data["contexts"]]
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nê²°ê³¼ ì €ì¥: {output_path}")
    
    def compare_chunking_strategies(self) -> Dict:
        """
        êµ¬ì¡° ì²­í‚¹ vs ì¼ë°˜ ì²­í‚¹ ë¹„êµ
        
        Returns:
            ë¹„êµ ê²°ê³¼
        """
        logger.info("\n=== ì²­í‚¹ ì „ëµ ë¹„êµ í‰ê°€ ===")
        
        results = {}
        
        # 1. êµ¬ì¡° ìš°ì„  ì²­í‚¹
        logger.info("\n[1/2] êµ¬ì¡° ìš°ì„  ì²­í‚¹ í‰ê°€")
        self.pipeline = RAGPipeline(use_structure_chunking=True)
        results["structure_chunking"] = self.evaluate(save_results=False)
        
        # 2. ì¼ë°˜ ì²­í‚¹
        logger.info("\n[2/2] ì¼ë°˜ ì²­í‚¹ í‰ê°€")
        self.pipeline = RAGPipeline(use_structure_chunking=False)
        results["general_chunking"] = self.evaluate(save_results=False)
        
        # ë¹„êµ ì¶œë ¥
        self._print_comparison(results)
        
        # ë¹„êµ ê²°ê³¼ ì €ì¥
        output_path = Path("tests/chunking_comparison.json")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        logger.info(f"\në¹„êµ ê²°ê³¼ ì €ì¥: {output_path}")
        
        return results
    
    def _print_comparison(self, results: Dict):
        """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*70)
        print("ì²­í‚¹ ì „ëµ ë¹„êµ ê²°ê³¼")
        print("="*70)
        
        structure = results["structure_chunking"]["metrics"]
        general = results["general_chunking"]["metrics"]
        
        print(f"\n{'ì§€í‘œ':25s} {'êµ¬ì¡° ì²­í‚¹':>12s} {'ì¼ë°˜ ì²­í‚¹':>12s} {'ì°¨ì´':>12s}")
        print("-"*70)
        
        for metric in structure.keys():
            s_score = structure[metric]
            g_score = general[metric]
            diff = s_score - g_score
            
            diff_str = f"+{diff:.3f}" if diff > 0 else f"{diff:.3f}"
            winner = "ğŸ†" if diff > 0 else ""
            
            print(f"{metric:25s} {s_score:>12.3f} {g_score:>12.3f} {diff_str:>12s} {winner}")
        
        print("-"*70)
        
        s_avg = results["structure_chunking"].get("average_score", 0)
        g_avg = results["general_chunking"].get("average_score", 0)
        avg_diff = s_avg - g_avg
        
        print(f"{'í‰ê· ':25s} {s_avg:>12.3f} {g_avg:>12.3f} {avg_diff:+12.3f}")
        print("="*70)
        
        # ê²°ë¡ 
        print("\n[ê²°ë¡ ]")
        if avg_diff > 0.05:
            print(f"âœ… êµ¬ì¡° ìš°ì„  ì²­í‚¹ì´ {avg_diff:.3f}ì  ë” ìš°ìˆ˜í•©ë‹ˆë‹¤!")
        elif avg_diff < -0.05:
            print(f"âš ï¸ ì¼ë°˜ ì²­í‚¹ì´ {abs(avg_diff):.3f}ì  ë” ìš°ìˆ˜í•©ë‹ˆë‹¤.")
        else:
            print("ë‘ ì „ëµì˜ ì„±ëŠ¥ì´ ë¹„ìŠ·í•©ë‹ˆë‹¤.")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="RAGAS RAG í‰ê°€")
    parser.add_argument(
        "--mode",
        choices=["evaluate", "compare", "simple"],
        default="evaluate",
        help="í‰ê°€ ëª¨ë“œ: evaluate (ì „ì²´ í‰ê°€), compare (ì²­í‚¹ ë¹„êµ), simple (ê´€ë ¨ì„±+ì†ë„ë§Œ)"
    )
    parser.add_argument(
        "--test-cases",
        default="tests/test_cases.json",
        help="í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ JSON íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--metrics",
        nargs="+",
        choices=["faithfulness", "answer_relevancy", "context_precision", "context_recall", "answer_correctness"],
        help="í‰ê°€í•  ì§€í‘œ ì„ íƒ (ë¯¸ì§€ì • ì‹œ answer_relevancyë§Œ)"
    )
    parser.add_argument(
        "--no-response-time",
        action="store_true",
        help="ì‘ë‹µ ì‹œê°„ ì¸¡ì • ì œì™¸"
    )
    
    args = parser.parse_args()
    
    try:
        evaluator = RAGASEvaluator(test_cases_path=args.test_cases)
        
        # ì§€í‘œ ì„ íƒ
        selected_metrics = None
        if args.metrics:
            metric_map = {
                "faithfulness": faithfulness,
                "answer_relevancy": answer_relevancy,
                "context_precision": context_precision,
                "context_recall": context_recall,
                "answer_correctness": answer_correctness
            }
            selected_metrics = [metric_map[m] for m in args.metrics]
        
        if args.mode == "simple":
            # ê°„ë‹¨í•œ í‰ê°€ (ì‹ ë¢°ì„± + ê´€ë ¨ì„± + ì‘ë‹µ ì†ë„)
            print("\n[ê°„ë‹¨í•œ í‰ê°€ ëª¨ë“œ: ì‹ ë¢°ì„± + ê´€ë ¨ì„± + ì‘ë‹µ ì†ë„]\n")
            evaluator.evaluate(
                metrics=[faithfulness, answer_relevancy],
                include_response_time=True
            )
        elif args.mode == "evaluate":
            # ê¸°ë³¸ í‰ê°€
            evaluator.evaluate(
                metrics=selected_metrics,
                include_response_time=not args.no_response_time
            )
        else:
            # ì²­í‚¹ ì „ëµ ë¹„êµ
            evaluator.compare_chunking_strategies()
            
    except ImportError as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        print("\nì„¤ì¹˜ ëª…ë ¹ì–´:")
        print("  pip install ragas datasets")
        return 1
    except Exception as e:
        print(f"\nâŒ í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())

