"""
RAGAS ê¸°ë°˜ RAG ì‹œìŠ¤í…œ í‰ê°€ í…ŒìŠ¤íŠ¸
"""

import os
import sys
import json
import time
import pytest
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rag.pipeline import RAGPipeline
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()


class TestRAGASEvaluation:
    """RAGAS í‰ê°€ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    @pytest.fixture(scope="class")
    def pipeline(self):
        """RAG íŒŒì´í”„ë¼ì¸ fixture"""
        return RAGPipeline()
    
    @pytest.fixture
    def test_cases(self):
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ"""
        test_cases_file = project_root / "tests" / "test_cases_real.json"
        
        if not test_cases_file.exists():
            pytest.skip("í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        with open(test_cases_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def test_simple_mode(self, pipeline, test_cases):
        """
        ê°„ë‹¨ ëª¨ë“œ: Faithfulness, Answer Relevancy, Response Timeë§Œ í‰ê°€
        """
        print("\n" + "="*80)
        print("  ğŸ“Š RAGAS í‰ê°€ - ê°„ë‹¨ ëª¨ë“œ")
        print("="*80)
        
        # ì²« 3ê°œ ì§ˆë¬¸ë§Œ í…ŒìŠ¤íŠ¸
        sample_cases = test_cases[:3]
        
        results_data = {
            "question": [],
            "answer": [],
            "contexts": [],
            "ground_truth": [],
        }
        
        response_times = []
        
        for item in sample_cases:
            question = item['question']
            ground_truth = item.get('ground_truth', '')
            
            print(f"\nì§ˆë¬¸: {question}")
            
            start_time = time.time()
            result = pipeline.query(question, top_k=5)
            elapsed_time = time.time() - start_time
            response_times.append(elapsed_time)
            
            answer = result.get('answer', '')
            contexts = [doc.page_content for doc in result.get('source_documents', [])]
            
            results_data['question'].append(question)
            results_data['answer'].append(answer)
            results_data['contexts'].append(contexts)
            results_data['ground_truth'].append(ground_truth)
            
            print(f"ë‹µë³€: {answer[:100]}...")
            print(f"ì‘ë‹µ ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
        
        # RAGAS í‰ê°€
        dataset = Dataset.from_dict(results_data)
        metrics = [faithfulness, answer_relevancy]
        
        evaluation_results = evaluate(dataset, metrics=metrics)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("  í‰ê°€ ê²°ê³¼")
        print("="*80)
        
        for metric_name, score in evaluation_results.items():
            if not metric_name.startswith('_'):
                print(f"{metric_name:25s}: {score:.3f}")
        
        avg_response_time = sum(response_times) / len(response_times)
        print(f"{'í‰ê·  ì‘ë‹µ ì‹œê°„':25s}: {avg_response_time:.2f}ì´ˆ")
        
        # Assertion
        assert evaluation_results['faithfulness'] > 0.5, "Faithfulnessê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤"
        assert evaluation_results['answer_relevancy'] > 0.5, "Answer Relevancyê°€ ë„ˆë¬´ ë‚®ìŠµë‹ˆë‹¤"
        assert avg_response_time < 5.0, "ì‘ë‹µ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤"
    
    def test_full_evaluation(self, pipeline, test_cases):
        """
        ì „ì²´ í‰ê°€: ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì— ëŒ€í•´ í‰ê°€
        """
        print("\n" + "="*80)
        print("  ğŸ“Š RAGAS í‰ê°€ - ì „ì²´ ëª¨ë“œ")
        print("="*80)
        
        results_data = {
            "question": [],
            "answer": [],
            "contexts": [],
            "ground_truth": [],
        }
        
        response_times = []
        
        for i, item in enumerate(test_cases, 1):
            question = item['question']
            ground_truth = item.get('ground_truth', '')
            
            print(f"\n[{i}/{len(test_cases)}] {question}")
            
            try:
                start_time = time.time()
                result = pipeline.query(question, top_k=5)
                elapsed_time = time.time() - start_time
                response_times.append(elapsed_time)
                
                answer = result.get('answer', '')
                contexts = [doc.page_content for doc in result.get('source_documents', [])]
                
                results_data['question'].append(question)
                results_data['answer'].append(answer)
                results_data['contexts'].append(contexts)
                results_data['ground_truth'].append(ground_truth)
                
                print(f"  âœ… ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
                
            except Exception as e:
                print(f"  âŒ ì‹¤íŒ¨: {e}")
                results_data['question'].append(question)
                results_data['answer'].append("")
                results_data['contexts'].append([])
                results_data['ground_truth'].append(ground_truth)
        
        # RAGAS í‰ê°€
        dataset = Dataset.from_dict(results_data)
        metrics = [faithfulness, answer_relevancy]
        
        evaluation_results = evaluate(dataset, metrics=metrics)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*80)
        print("  ì „ì²´ í‰ê°€ ê²°ê³¼")
        print("="*80)
        
        for metric_name, score in evaluation_results.items():
            if not metric_name.startswith('_'):
                emoji = "âœ…" if score >= 0.7 else "âš ï¸" if score >= 0.5 else "âŒ"
                print(f"{metric_name:25s}: {score:.3f}  {emoji}")
        
        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            print(f"{'í‰ê·  ì‘ë‹µ ì‹œê°„':25s}: {avg_response_time:.2f}ì´ˆ")
        
        # ê²°ê³¼ ì €ì¥
        output_file = project_root / "tests" / "ragas_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                "evaluation_results": dict(evaluation_results),
                "avg_response_time": avg_response_time if response_times else None,
                "total_questions": len(test_cases),
                "successful_queries": len(response_times)
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # Assertion
        assert evaluation_results['faithfulness'] > 0.6, "ì „ì²´ Faithfulnessê°€ ë‚®ìŠµë‹ˆë‹¤"
        assert evaluation_results['answer_relevancy'] > 0.6, "ì „ì²´ Answer Relevancyê°€ ë‚®ìŠµë‹ˆë‹¤"
    
    def test_compare_modes(self, pipeline):
        """
        ë¹„êµ ëª¨ë“œ: êµ¬ì¡° ì²­í‚¹ vs ì¼ë°˜ ì²­í‚¹ ë¹„êµ
        """
        print("\n" + "="*80)
        print("  ğŸ“Š RAGAS í‰ê°€ - ë¹„êµ ëª¨ë“œ")
        print("="*80)
        
        # ìƒ˜í”Œ ì§ˆë¬¸
        sample_questions = [
            {
                "question": "ê°ì‚¬ëŠ” ëˆ„êµ¬ì—ê²Œ ë³´ê³ í•˜ë‚˜ìš”?",
                "ground_truth": "ê°ì‚¬ëŠ” ëŒ€í‘œì´ì‚¬ ë˜ëŠ” ì´ì‚¬íšŒì— ì§ì ‘ ë³´ê³ í•©ë‹ˆë‹¤."
            },
            {
                "question": "ê³„ì•½ ì²´ê²° ì‹œ ì „ê²°ê¶ŒìëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
                "ground_truth": "ê³„ì•½ ê¸ˆì•¡ê³¼ ìœ í˜•ì— ë”°ë¼ ìœ„ì„ì „ê²°ê·œì¹™ì—ì„œ ì •í•œ ì§ê¸‰ì˜ ë‹´ë‹¹ìê°€ ì „ê²°ê¶Œìì…ë‹ˆë‹¤."
            }
        ]
        
        # êµ¬ì¡° ì²­í‚¹ìœ¼ë¡œ í‰ê°€
        print("\n--- êµ¬ì¡° ì²­í‚¹ í‰ê°€ ---")
        structure_results = self._evaluate_with_mode(pipeline, sample_questions, use_structure=True)
        
        # ì¼ë°˜ ì²­í‚¹ìœ¼ë¡œ í‰ê°€ (ë¹„êµë¥¼ ìœ„í•´ íŒŒì´í”„ë¼ì¸ ì¬ìƒì„±)
        print("\n--- ì¼ë°˜ ì²­í‚¹ í‰ê°€ ---")
        pipeline_general = RAGPipeline(use_structure_chunking=False)
        general_results = self._evaluate_with_mode(pipeline_general, sample_questions, use_structure=False)
        
        # ë¹„êµ ì¶œë ¥
        print("\n" + "="*80)
        print("  ë¹„êµ ê²°ê³¼")
        print("="*80)
        print(f"{'ì§€í‘œ':25s} | {'êµ¬ì¡° ì²­í‚¹':>12s} | {'ì¼ë°˜ ì²­í‚¹':>12s} | {'ì°¨ì´':>10s}")
        print("-" * 80)
        
        for metric in ['faithfulness', 'answer_relevancy']:
            if metric in structure_results and metric in general_results:
                struct_score = structure_results[metric]
                general_score = general_results[metric]
                diff = struct_score - general_score
                diff_str = f"+{diff:.3f}" if diff > 0 else f"{diff:.3f}"
                
                print(f"{metric:25s} | {struct_score:12.3f} | {general_score:12.3f} | {diff_str:>10s}")
        
        # Assertion
        assert structure_results['faithfulness'] >= general_results['faithfulness'], \
            "êµ¬ì¡° ì²­í‚¹ì´ ì¼ë°˜ ì²­í‚¹ë³´ë‹¤ Faithfulnessê°€ ë‚®ìŠµë‹ˆë‹¤"
    
    def _evaluate_with_mode(self, pipeline, questions, use_structure=True):
        """íŠ¹ì • ëª¨ë“œë¡œ í‰ê°€ ì‹¤í–‰"""
        results_data = {
            "question": [],
            "answer": [],
            "contexts": [],
            "ground_truth": [],
        }
        
        for item in questions:
            question = item['question']
            ground_truth = item.get('ground_truth', '')
            
            result = pipeline.query(question, top_k=5)
            answer = result.get('answer', '')
            contexts = [doc.page_content for doc in result.get('source_documents', [])]
            
            results_data['question'].append(question)
            results_data['answer'].append(answer)
            results_data['contexts'].append(contexts)
            results_data['ground_truth'].append(ground_truth)
        
        dataset = Dataset.from_dict(results_data)
        metrics = [faithfulness, answer_relevancy]
        
        return evaluate(dataset, metrics=metrics)


def test_ragas_metrics_available():
    """RAGAS ë©”íŠ¸ë¦­ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ëŠ”ì§€ í™•ì¸"""
    assert faithfulness is not None
    assert answer_relevancy is not None
    print("\nâœ… RAGAS ë©”íŠ¸ë¦­ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    # pytest ì‹¤í–‰
    pytest.main([__file__, "-v", "-s"])

