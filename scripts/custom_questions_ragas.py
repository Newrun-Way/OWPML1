"""
ì»¤ìŠ¤í…€ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì¼ì—ì„œ ì½ì–´ RAGAS í‰ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from rag.pipeline import RAGPipeline
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv()


def load_questions_from_file(file_path: str):
    """
    JSON íŒŒì¼ì—ì„œ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
    
    íŒŒì¼ í˜•ì‹:
    [
        {
            "question": "ì§ˆë¬¸ ë‚´ìš©",
            "ground_truth": "ì •ë‹µ (ì„ íƒì‚¬í•­)"
        },
        ...
    ]
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        questions = json.load(f)
    
    return questions


def evaluate_custom_questions(questions_file: str, output_file: str = None):
    """ì»¤ìŠ¤í…€ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¡œ RAGAS í‰ê°€"""
    
    print("="*80)
    print("  ğŸ“‹ ì»¤ìŠ¤í…€ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ RAGAS í‰ê°€")
    print("="*80)
    print()
    
    # ì§ˆë¬¸ ë¡œë“œ
    print(f"ğŸ“‚ ì§ˆë¬¸ íŒŒì¼ ë¡œë”©: {questions_file}")
    try:
        questions_data = load_questions_from_file(questions_file)
        print(f"âœ… {len(questions_data)}ê°œì˜ ì§ˆë¬¸ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n")
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {questions_file}")
        return
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    print("ğŸ“¦ RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
    try:
        pipeline = RAGPipeline()
        print("âœ… íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ íŒŒì´í”„ë¼ì¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ê° ì§ˆë¬¸ì— ëŒ€í•´ RAG ì‹œìŠ¤í…œ ì‹¤í–‰
    results_data = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": [],
    }
    
    print("="*80)
    print("ğŸ” RAG ì‹œìŠ¤í…œìœ¼ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")
    print("="*80)
    print()
    
    for i, item in enumerate(questions_data, 1):
        question = item['question']
        ground_truth = item.get('ground_truth', '')
        
        print(f"[{i}/{len(questions_data)}] {question}")
        
        try:
            start_time = time.time()
            result = pipeline.query(question, top_k=5)
            elapsed_time = time.time() - start_time
            
            answer = result.get('answer', '')
            contexts = [doc.page_content for doc in result.get('source_documents', [])]
            
            results_data['question'].append(question)
            results_data['answer'].append(answer)
            results_data['contexts'].append(contexts)
            results_data['ground_truth'].append(ground_truth)
            
            print(f"  âœ… ì™„ë£Œ ({elapsed_time:.2f}ì´ˆ)")
            print(f"  ë‹µë³€: {answer[:100]}...")
            print()
            
        except Exception as e:
            print(f"  âŒ ì‹¤íŒ¨: {e}\n")
            # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
            results_data['question'].append(question)
            results_data['answer'].append("")
            results_data['contexts'].append([])
            results_data['ground_truth'].append(ground_truth)
    
    # RAGAS í‰ê°€
    print("="*80)
    print("âš–ï¸  RAGAS í‰ê°€ ì‹¤í–‰ ì¤‘...")
    print("="*80)
    print()
    
    try:
        dataset = Dataset.from_dict(results_data)
        
        # í‰ê°€ ì§€í‘œ
        metrics = [faithfulness, answer_relevancy]
        
        # í‰ê°€ ì‹¤í–‰
        evaluation_results = evaluate(dataset, metrics=metrics)
        
        # ê²°ê³¼ ì¶œë ¥
        print("="*80)
        print("  ğŸ“Š RAGAS í‰ê°€ ê²°ê³¼")
        print("="*80)
        print()
        
        for metric_name, score in evaluation_results.items():
            if metric_name.startswith('_'):
                continue
            
            if score >= 0.8:
                emoji = "âœ…"
                status = "ìš°ìˆ˜"
            elif score >= 0.6:
                emoji = "âš ï¸"
                status = "ë³´í†µ"
            else:
                emoji = "âŒ"
                status = "ê°œì„  í•„ìš”"
            
            print(f"{metric_name:25s}: {score:.3f}  {emoji} {status}")
        
        print()
        print("="*80)
        
        # ê²°ê³¼ ì €ì¥
        if output_file:
            output_data = {
                "evaluation_results": dict(evaluation_results),
                "details": []
            }
            
            for i in range(len(results_data['question'])):
                output_data['details'].append({
                    "question": results_data['question'][i],
                    "answer": results_data['answer'][i],
                    "ground_truth": results_data['ground_truth'][i],
                    "contexts": results_data['contexts'][i]
                })
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            print(f"\nğŸ’¾ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}")
        
    except Exception as e:
        print(f"âŒ RAGAS í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


def main():
    parser = argparse.ArgumentParser(
        description="ì»¤ìŠ¤í…€ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¡œ RAGAS í‰ê°€"
    )
    parser.add_argument(
        "--file",
        type=str,
        default="my_questions.json",
        help="ì§ˆë¬¸ì´ ë‹´ê¸´ JSON íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="í‰ê°€ ê²°ê³¼ë¥¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)"
    )
    
    args = parser.parse_args()
    
    evaluate_custom_questions(args.file, args.output)


if __name__ == "__main__":
    main()

